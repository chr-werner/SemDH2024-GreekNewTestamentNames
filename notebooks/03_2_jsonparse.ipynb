{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Add data manuscript data from NTVMR metadata files\n",
    "## 1 Installing needed packages"
   ],
   "id": "4b9e5e548fb63072"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "!pip install --quiet pandas==2.1.4\n",
    "!pip install --quiet tqdm==4.66.4\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from glob import glob\n",
    "from converters import docID_to_ga\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# read file manuscripts_tei.csv\n",
    "manuscripts_tei_df = pd.read_csv(\n",
    "    \"../data/manuscripts_tei.csv\",\n",
    "    low_memory=False,\n",
    "    usecols=[\"docID\", \"ga\", \"label\", \"source\"],\n",
    "    dtype={\n",
    "        \"docID\": \"string\",\n",
    "        \"ga\": \"string\",\n",
    "        \"label\": \"string\",\n",
    "        \"source\": \"string\",\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1716d132bbbbd596",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 Process JSON files in parallel\n",
    "\n",
    "Currently, we only take docID, ga, pagesCount, and estimated originYear/century. BUT there is way more data for each page/image etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "38d7cd601739d268",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function to process each JSON file and extract information\n",
    "def process_json_file(file_path):\n",
    "    \"\"\"Process given JSON file and return relevant data from it\n",
    "\n",
    "    TODO: it might be easier to work with xml files, which can be selected whiles downloading the manuscript metadata\n",
    "\n",
    "    :param file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    # Initialize all values to NaN\n",
    "    docID = century = pagesCount = ga = leavesCount = float(\"nan\")\n",
    "\n",
    "    # try to fill variables with values\n",
    "    docID = json_data[\"data\"][\"manuscript\"][\"docID\"]\n",
    "    # originYearLate = json_data[\"data\"][\"manuscript\"][\"originYear\"][\"late\"]\n",
    "    # originYearEarly = json_data[\"data\"][\"manuscript\"][\"originYear\"][\"early\"]\n",
    "    pagesCount = json_data[\"data\"][\"manuscript\"][\"pages\"][\"count\"]\n",
    "    leavesCount = json_data[\"data\"][\"manuscript\"][\"leaves\"][\"leavesCount\"]\n",
    "    ga = docID_to_ga(\n",
    "        docID\n",
    "    )  # BUG: json_data[\"data\"][\"manuscript\"][\"leaves\"][\"gaNum\"] for 20000 and 30000 identical FIX: wrote own function\n",
    "\n",
    "    # sometimes no shelf index is given\n",
    "    # try:\n",
    "    #    shelf_instances_data = json_data[\"data\"][\"manuscript\"][\"shelfInstances\"][\n",
    "    #        \"shelfInstance\"\n",
    "    #    ]\n",
    "    #    if isinstance(shelf_instances_data, list):\n",
    "    #        shelf_instances = [\n",
    "    #            {k: v for k, v in instance.items() if v != \"\"}\n",
    "    #            for instance in shelf_instances_data\n",
    "    #        ]\n",
    "    #    elif isinstance(\n",
    "    #        shelf_instances_data, dict\n",
    "    #    ):  # Clean and convert a single dictionary to a list of dictionaries\n",
    "    #        shelf_instances = [\n",
    "    #            {k: v for k, v in shelf_instances_data.items() if v != \"\"}\n",
    "    #        ]\n",
    "    #    else:\n",
    "    #        shelf_instances = []\n",
    "    # except:\n",
    "    #    shelf_instances = None\n",
    "\n",
    "    # sometimes century is not given\n",
    "    try:\n",
    "        century = json_data[\"data\"][\"manuscript\"][\"originYear\"][\"content\"]\n",
    "    except:\n",
    "        century = None\n",
    "\n",
    "    return {\n",
    "        \"docID\": docID,\n",
    "        # \"originYearLate\": originYearLate,\n",
    "        # \"originYearEarly\": originYearEarly,\n",
    "        \"pagesCount\": pagesCount,\n",
    "        \"leavesCount\": leavesCount,\n",
    "        \"ga\": ga,\n",
    "        \"century\": century,\n",
    "        # \"shelfInstances\": shelf_instances,\n",
    "        \"source\": \"ntvmr\",\n",
    "    }\n",
    "\n",
    "\n",
    "# List of JSON files\n",
    "json_files = glob(\"../data/manuscripts/ntvmr/*.json\")\n",
    "\n",
    "# Use ProcessPoolExecutor with tqdm progress bar\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Wrap the executor.map call with tqdm for the progress bar\n",
    "    results = list(\n",
    "        tqdm(executor.map(process_json_file, json_files), total=len(json_files))\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "manuscripts_json_df = pd.DataFrame(results)\n",
    "manuscripts_json_df.to_csv(\"../data/manuscripts_json.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ba7d8caf4e2b02e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 Merge\n",
    "Here we merge the just generated data with the already found data from teiparse.ipynb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# concat multiple dataframes\n",
    "merged_df = pd.concat([manuscripts_json_df, manuscripts_tei_df], ignore_index=True)"
   ],
   "id": "ca5962445795e9a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "287e0fc12efacfaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4 Writing to file\n",
    "\n",
    "Before writing to file pagesCount should be set to be integer, therefor NaN values are set to 0"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "column_types = {\n",
    "    \"docID\": \"string\",\n",
    "    \"pagesCount\": \"Int64\",\n",
    "    \"leavesCount\": \"Int64\",\n",
    "    \"ga\": \"string\",\n",
    "    \"century\": \"string\",\n",
    "    \"source\": \"string\",\n",
    "    \"label\": \"string\",\n",
    "}\n",
    "merged_df.astype(column_types)\n",
    "\n",
    "merged_df.to_csv(\"../data/manuscripts_json_tei.csv\", index=False)"
   ],
   "id": "6ac332c929b8cc72",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
