{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing TEI files\n",
    "\n",
    "## 1 Installing needed packages\n",
    "\n",
    "When running on a remote JupyterLab, packages that are needed have to be explicitly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install beautifulsoup4==4.12.3\n",
    "!{sys.executable} -m pip install lxml==5.0.0\n",
    "!{sys.executable} -m pip install pandas==2.1.4\n",
    "!{sys.executable} -m pip install sparqlwrapper==2.0.0\n",
    "\n",
    "from xml.sax.handler import ContentHandler\n",
    "from xml.sax import make_parser\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "from utils import (\n",
    "    check_xml,\n",
    "    get_data_from_tei,\n",
    "    fix_bkv,\n",
    "    generate_transcription_url,\n",
    ")\n",
    "\n",
    "from converters import bkv_to_nkv, ga_to_docID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Parsing\n",
    "\n",
    "For parsing BeautifulSoup is used, as it is fast and reliable.\n",
    "There are some dataclasses in use, which handle the objects 'Source', 'Verse' and 'Manuscript'. This is done to make the code more readable and easier to maintain. These classes are defined in `TEIFile.py`. \n",
    "The TEIFile class is used to work on/extract data from a given TEI file. The class is defined in `TEIFile.py`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Get data from TEI files\n",
    "\n",
    "One goal is to extract all verses from all transcriptions (from NTVMR and IGNTP). This is done with the following code blocks.\n",
    "\n",
    "### 3.1 Check files for validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time\n",
    "tic = time.time()\n",
    "\n",
    "# xml parser init\n",
    "parser = make_parser()\n",
    "parser.setContentHandler(ContentHandler())\n",
    "\n",
    "# get files\n",
    "raw_files = sorted(Path(\"../data/transcriptions/\").rglob(\"*.xml\"))\n",
    "#raw_files = [Path(\"../data/transcriptions/igntp/ecm_1corinthians/NT_GRC_1_1Cor.xml\")]\n",
    "\n",
    "# Store paths to good files\n",
    "well_formed_files = []\n",
    "\n",
    "# Execute tasks and gather results\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks and collect futures\n",
    "    futures = [executor.submit(check_xml, file_path, parser) for file_path in raw_files]\n",
    "\n",
    "    # Gather results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                well_formed_files.append(result)  # Append each result to the list\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")  # Handle exceptions here\n",
    "\n",
    "# Record the end time\n",
    "toc = time.time()\n",
    "# Calculate the runtime\n",
    "runtime = toc - tic\n",
    "print(f\"The script took {runtime:.4f} seconds to execute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malformed_files = [x for x in raw_files if x not in well_formed_files]\n",
    "\n",
    "print(malformed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get data from TEI file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Data extraction from multiple TEI files in parallel\n",
    "\n",
    "As there are many TEI files, it is necessary (for speed) to run the extraction of data in parallel. Use 'max_workers' to set number of cpu cores to be utilised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time\n",
    "tic = time.time()\n",
    "\n",
    "# Store results\n",
    "manuscripts_list = []\n",
    "verses_list = []\n",
    "\n",
    "# Execute tasks and gather results\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=16) as executor:\n",
    "    # Submit tasks and collect futures\n",
    "    futures = [\n",
    "        executor.submit(get_data_from_tei, file_path) for file_path in well_formed_files\n",
    "    ]\n",
    "\n",
    "    # Gather results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            manuscript, verses = future.result()\n",
    "            manuscripts_list.append(manuscript)  # Append each result to the list\n",
    "            verses_list.append(verses)  # Append each result to the list\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")  # Handle exceptions here\n",
    "\n",
    "# Record the end time\n",
    "toc = time.time()\n",
    "# Calculate the runtime\n",
    "runtime = toc - tic\n",
    "print(f\"The script took {runtime:.4f} seconds to execute.\")\n",
    "\n",
    "# takes ~4min with 16 cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Lists to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening list of verses\n",
    "flattened_verses_list = []\n",
    "# Flattening the list of lists of dictionaries\n",
    "for sublist in verses_list:\n",
    "    flattened_verses_list.extend(sublist)\n",
    "\n",
    "# list of dicts to data frame\n",
    "verses_df = pd.DataFrame(flattened_verses_list)\n",
    "manuscripts_df = pd.DataFrame(manuscripts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Apply cleanup and aggregation functions to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Better check if nkv or bkv, set it to appropriate column, then generate the other column\n",
    "# Apply the conversion function to the 'bkv' column in the DataFrame\n",
    "verses_df[\"bkv\"] = verses_df.apply(fix_bkv, axis=1)\n",
    "verses_df[\"docID\"] = verses_df.apply(ga_to_docID, axis=1)\n",
    "verses_df[\"nkv\"] = verses_df.apply(bkv_to_nkv, axis=1)\n",
    "verses_df[\"ntvmrLink\"] = verses_df.apply(generate_transcription_url, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manuscripts_df[\"docID\"] = manuscripts_df.apply(ga_to_docID, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Grouping\n",
    "currently we do not want any grouping, to keep data seperated by its sources (here igntp and ntvmr, later also dbpedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manuscripts_df[\"docID\"] = manuscripts_df[\"docID\"].astype(int)\n",
    "# Setting the type for \"int\" has to be done as otherwise docID could be of different type (e.g. \"10001\",10001,10001.0). With different types it can't be grouped.\n",
    "\n",
    "# Merge rows by docID\n",
    "# manuscripts_df = (\n",
    "#    manuscripts_df.groupby(\"docID\")\n",
    "#    .agg(\n",
    "#        {\n",
    "#            \"ga\": lambda x: \" \".join(set(x)),\n",
    "#            \"label\": lambda x: \" \".join(set(x)),\n",
    "#            \"source\": lambda x: \",\".join(set(x)),\n",
    "#        }\n",
    "#    )\n",
    "#    .reset_index()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2.5 Dropping rows\n",
    "\n",
    "Some rows in verses_df do not contain a bkv – marked with NONE by fix_bkv() – as they are an inscriptio or a subscriptio. We drop those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verses_df.dropna(subset=[\"bkv\"], inplace=True)\n",
    "verses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Save data frames to CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses_df.sort_values(by=[\"docID\", \"bkv\"], inplace=True)\n",
    "# verses_df.drop_duplicates(subset=[\"docID\", \"bkv\"], inplace=True)\n",
    "verses_df.to_csv(\"../data/verses.csv\", index=False, index_label=\"index\")\n",
    "# verses_df.to_json(\"../data/verses.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manuscripts_df.sort_values(by=\"docID\", inplace=True)\n",
    "manuscripts_df.drop_duplicates(inplace=True)\n",
    "manuscripts_df.to_csv(\"../data/manuscripts_tei.csv\", index=False, index_label=\"index\")\n",
    "# manuscripts_df.to_json(\"../data/manuscripts.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
