{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing TEI files\n",
    "\n",
    "## 1 Installing needed packages\n",
    "\n",
    "When running on a remote JupyterLab, packages that are needed have to be explicitly installed:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "!pip install --quiet beautifulsoup4==4.12.3\n",
    "!pip install --quiet lxml==5.0.0\n",
    "!pip install --quiet pandas==2.1.4\n",
    "!pip install --quiet sparqlwrapper==2.0.0\n",
    "!pip install --quiet python-dateutil==2.9.0.post0\n",
    "!pip install --quiet pyarrow==16.0.0\n",
    "!pip install --quiet tqdm==4.66.4\n",
    "\n",
    "from xml.sax.handler import ContentHandler\n",
    "from xml.sax import make_parser\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from utils import (\n",
    "    check_xml,\n",
    "    bkv_nkv_from_verse_id,\n",
    "    gap_clean,\n",
    "    get_data_from_tei,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Parsing\n",
    "\n",
    "For parsing BeautifulSoup is used, as it is fast and reliable.\n",
    "There are some dataclasses in use, which handle the objects 'Source', 'Verse' and 'Manuscript'. This is done to make the code more readable and easier to maintain. These classes are defined in `TEIFile.py`. \n",
    "The TEIFile class is used to work on/extract data from a given TEI file. The class is defined in `TEIFile.py`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Get data from TEI files\n",
    "\n",
    "One goal is to extract all verses from all transcriptions (from NTVMR and IGNTP). This is done with the following code blocks.\n",
    "\n",
    "### 3.1 Check files for validity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# xml parser init\n",
    "parser = make_parser()\n",
    "parser.setContentHandler(ContentHandler())\n",
    "\n",
    "# get files\n",
    "raw_files = sorted(Path(\"../data/transcriptions\").rglob(\"*.xml\"))\n",
    "# raw_files = sorted(Path(\"../data/transcriptions/igntp/ecm_romans\").rglob(\"*.xml\"))\n",
    "# raw_files = [Path(\"../data/transcriptions/ntvmr/40211.xml\")]\n",
    "\n",
    "# Store paths to good files\n",
    "well_formed_files = []\n",
    "\n",
    "# Execute tasks and gather results\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks and collect futures\n",
    "    futures = [executor.submit(check_xml, file_path, parser) for file_path in raw_files]\n",
    "\n",
    "    # Gather results\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                well_formed_files.append(result)  # Append each result to the list\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")  # Handle exceptions here"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "malformed_files = [x for x in raw_files if x not in well_formed_files]\n",
    "\n",
    "print(malformed_files)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get data from TEI file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Data extraction from multiple TEI files in parallel\n",
    "\n",
    "As there are many TEI files, it is necessary (for speed) to run the extraction of data in parallel. Use 'max_workers' to set number of cpu cores to be utilised."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create the directories if it doesn't exist\n",
    "out_dirs = [\"../data/parsed/man\", \"../data/parsed/trans\"]\n",
    "os.makedirs(out_dirs[0], exist_ok=True)\n",
    "os.makedirs(out_dirs[1], exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T07:49:52.073921Z",
     "start_time": "2024-07-08T07:49:51.935783Z"
    }
   },
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Execute tasks and gather results\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Submit tasks and collect futures\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            get_data_from_tei,\n",
    "            file_path,\n",
    "            clear_only=True,\n",
    "            verbose=False,\n",
    "            write_to_file=True,\n",
    "            trans_out_dir=out_dirs[1],\n",
    "            man_out_dir=out_dirs[0],\n",
    "        )\n",
    "        for file_path in well_formed_files\n",
    "    ]\n",
    "\n",
    "    # Initialize tqdm progress bar with total number of tasks\n",
    "    progress_bar = tqdm(total=len(futures), desc=\"Processing\")\n",
    "\n",
    "    # Gather results\n",
    "    for future, file_path in zip(\n",
    "        concurrent.futures.as_completed(futures), well_formed_files\n",
    "    ):\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.update(1)\n",
    "        # Write currently running file path\n",
    "        # progress_bar.write(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Concatenating like this is done, as we know all files do have the same header.\n",
    "# Also, this is computationally more efficient than first reading each file into a pd.DataFrame and then merging those into one.\n",
    "\n",
    "# Construct the shell commands for concatenating\n",
    "command1 = f\"awk 'NR == 1 || FNR > 1' ../data/parsed/man/*.csv > ../data/parsed/manuscripts.csv\"\n",
    "command2 = (\n",
    "    f\"awk 'NR == 1 || FNR > 1' ../data/parsed/trans/*.csv > ../data/parsed/verses.csv\"\n",
    ")\n",
    "\n",
    "# Execute the shell commands\n",
    "!{command1}\n",
    "!{command2}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "verses_df = pd.read_csv(\n",
    "    \"../data/parsed/verses.csv\",\n",
    "    dtype={\n",
    "        \"lection\": \"string\",\n",
    "        \"verse\": \"string\",\n",
    "        \"transcript\": \"string\",\n",
    "        \"publisher\": \"string\",\n",
    "        \"source\": \"string\",\n",
    "        \"ga\": \"string\",\n",
    "        \"sponsor\": \"string\",\n",
    "        \"founder\": \"string\",\n",
    "        \"edition_version\": \"float\",\n",
    "        \"edition_date\": \"string\",\n",
    "        \"published_date\": \"string\",\n",
    "        \"encoding_version\": \"float\",\n",
    "    },\n",
    ")\n",
    "manuscripts_df = pd.read_csv(\n",
    "    \"../data/parsed/manuscripts.csv\",\n",
    "    dtype={\"ga\": \"string\", \"docID\": \"string\", \"label\": \"string\", \"source\": \"string\"},\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Apply cleanup and aggregation functions to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Apply the conversion function to the 'bkv' column in the DataFrame\n",
    "verses_df = verses_df.apply(bkv_nkv_from_verse_id, axis=1)\n",
    "verses_df.drop(columns=[\"verse\"], inplace=True)\n",
    "verses_df.dropna(subset=[\"transcript\"], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Measure the time it takes to apply the function\n",
    "start_time = time.time()\n",
    "# verses_df[\"ntvmrLink\"] = verses_df.apply(generate_transcription_url, axis=1)\n",
    "verses_df[\"text\"] = verses_df[\"transcript\"].apply(gap_clean)\n",
    "end_time = time.time()\n",
    "print(f\"{end_time - start_time:.4f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# manuscripts_df[\"docID\"] = manuscripts_df.apply(ga_to_docID, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": "#### 3.2.4 Dropping rows without verse identifier"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "verses_df.dropna(subset=[\"bkv\"], inplace=True)\n",
    "verses_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Save data frames to CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# sort by GA then by BKV\n",
    "verses_df.sort_values(by=[\"ga\", \"bkv\"], inplace=True)\n",
    "# add unique integer verse_id, as the transcription (or metadata like encoding_version or edition_version) can change over time\n",
    "verses_df[\"verse_id\"] = range(1, len(verses_df) + 1)\n",
    "# write to file\n",
    "verses_df.to_csv(\"../data/verses.csv\", index=False, index_label=\"index\")\n",
    "# verses_df.to_parquet(\"../data/verses.parquet\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "manuscripts_df.sort_values(by=\"ga\", inplace=True)\n",
    "manuscripts_df.drop_duplicates(inplace=True)\n",
    "manuscripts_df.to_csv(\"../data/manuscripts_tei.csv\", index=False, index_label=\"index\")\n",
    "# verses_df.to_parquet(\"../data/manuscripts_tei.parquet\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
